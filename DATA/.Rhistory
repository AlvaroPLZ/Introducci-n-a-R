datos_año <- subset(PIB_por_pai_ses, año_BM == año_2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, año_columna == año_2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, BM1 == año_2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, año_BM1 == año_2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, año_BM1 == año_2000)
library(readxl)
PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB por países.xls")
View(PIB_por_pai_ses)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, PIB_2020 == año_2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, año_columna== BM, año_deseado == 2000)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, columna== BM, año_deseado == 2000)
datos_año <- subset(PIB_por_pai_ses, BM, año_deseado == 2000)
library(readxl)
WhoGov_crosssectional_V1_1 <- read_excel("Desktop/Datos/WhoGov_crosssectional_V1.1.xlsx")
View(WhoGov_crosssectional_V1_1)
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, año == año_deseado)
datos_año <- subset(PIB_por_pai_ses, 2020 == año_deseado)
View(WhoGov_crosssectional_V1_1)
View(datos_año)
library(readxl)
PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB por países.xls")
View(PIB_por_pai_ses)
library(readxl)
> PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB por países.xls")
library(readxl)
> PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB por países.xls")
library(readxl)
PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB por países.xls")
# Seleccionar el año deseado
año_deseado <- 2020
datos_año <- subset(PIB_por_pai_ses, 2020 == año_deseado)
#Paso 2: Medidas de Tendencia Central
#   Media
mean(PIB_por_pai_ses)
#   Mediana
mediana <- median(datos)
#Paso 2: Medidas de Tendencia Central
#   Media
mean(2020)
#Paso 2: Medidas de Tendencia Central
#   Media
mean(datos_año)
library(readxl)
PIB_paises_2020 <- read_excel("Desktop/Datos/PIB_paises_2020.xls")
View(PIB_paises_2020)
library(readxl)
PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB_paises_2020s.xls")
PIB_por_pai_ses <- read_excel("Desktop/Datos/PIB_paises_2020.xls")
#Paso 2: Medidas de Tendencia Central
#   Media
mean(PIB_paises_2020)
datos_año <- subset(PIB_paises_2020, 2020)
datos_año <- subset(PIB_paises_2020, año_columna == 2020)
datos_año <- subset(PIB_paises_2020, 2020_B == 2020)
datos_año <- subset(PIB_paises_2020, 2020_B == 2020)
datos_año <- subset(PIB_paises_2020, año_B == año_2020)
datos_año <- subset(PIB_paises_2020, 2 == 2020)
View(datos_año)
datos_año <- subset(PIB_paises_2020, B == 2020)
datos_año <- subset(PIB_paises_2020, 2 == 2020)
View(datos_año)
datos_año <- subset(PIB_paises_2020, 1 == 2020)
nombre_de_lista(2)
PIB_paises_2020(2)
datos_año <- subset(PIB_paises_2020, año_B1 == 2020)
library(readxl)
PIB_paises_2020 <- read_excel("Desktop/Datos/PIB_paises_2020.xls")
View(PIB_paises_2020)
library(readxl)
Brain_size <- read_excel("Desktop/Datos/Brain size.xlsx")
View(Brain_size)
# Seleccionar la variable deseada
MRI_count <- Brain_size$MRI_Count
#Paso 2: Medidas de Tendencia Central
#   Media
media <- mean(MRI_count)
#   Mediana
mediana <- median(MRI_count)
#Moda
tabla_frecuencias <- table(MRI_count)
moda <- as.numeric(names(tabla_frecuencias[tabla_frecuencias == max(tabla_frecuencias)]))
#Paso 3: Medidas de Posición
# Primer cuartil
primer_cuartil <- quantile(MRI_count,0.25)
# Tercer cuartil
tercer_cuartil <- quantile(MRI_count, 0.75)
# Valor máximo
valor_maximo <- max(MRI_count)
# Valor mínimo
valor_minimo <- min(MRI_count)
#Paso 4: Medidas de Dispersión
# Amplitud
amplitud <- valor_maximo - valor_minimo
#Amplitud intercuarilica
iqr <- tercer_cuartil - primer_cuartil
# Varianza
varianza <- var(MRI_count)
# Desviación estándar
desviacion_estandar <- sd(MRI_count)
# Coeficiente de variación
coeficiente_variacion <- (desviacion_estandar/media)*100
#Paso 5: Gráficos
#Histograma
hist(MRI_count)
# Crear un histograma con más intervalos y colores personalizados
hist(datos, breaks = 10, col = "#FFE4E1", main = "Histograma de MRI_count", xlab = "Valores", ylab = "Frecuencia")
# Crear un histograma con más intervalos y colores personalizados
hist(MRI_count, breaks = 10, col = "#FFE4E1", main = "Histograma de MRI_count", xlab = "Valores", ylab = "Frecuencia")
#Diagrama de tallo y hoja
steam(MRI_count)
#Diagrama de tallo y hoja
stem(MRI_count)
#Diagrama de caja y brazos
boxplot(MRI_count)
#Diagrama de caja y brazos
boxplot(MRI_count, col="#FFE4E1")
install.packages("rmarkdown")
install.packages("tinytex")
tinytex::install_tinytex()
library(tinytex)
rnorm(10, mean = 10, sd=4)
rbinom(n=10, size =4, prob =0.3)
runif(5, 1, 10)
roud(runif(5,1,10))
round(runif(5,1,10))
rbinom(1,11,60/40)
rbinom(1,11,6/4)
rbinom(1,11,0.6)
rbinom(1,11,0.6)
num_ensayos <- 10000
num_monedas <- 11
prob_aguila <- 0.6
set.seed(2)
rbinom(1,11,0.6)
rbinom(num_ensayos, num_monedas, prob_aguila)
rbinom(num_ensayos, num_monedas, prob_aguila) == 4
(rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %<%
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
(rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %<%
sum()
(rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %>%
sum()
((rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %>%
sum())/num_ensayos
set.seed(2)
rbinom(num_ensayos, num_monedas, prob_aguila)
rbinom(num_ensayos, num_monedas, prob_aguila) == 4
(rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %>%
sum()
((rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %>%
sum())/num_ensayos
table(rbinom(num_ensayos,num_monedas, prob_aguila))
set.seed(2)
table(rbinom(num_ensayos,num_monedas, prob_aguila))
table(rbinom(num_ensayos,num_monedas, prob_aguila))/num_ensayos
hist(rbinom(num_ensayos,num_monedas, prob_aguila))
resultados_positivos <-
(rbinom(num_ensayos, num_monedas, prob_aguila) == 4) %>%
sum()
resultados_positivos/num_ensayos
dbinom(4,11,0.6)
pbinom(4,11,0.6)
tabla_prob <- (rbinom(num_ensayos, num_monedas, prob_aguila) %>%
table())/num_ensayos
tabla_prob[1:5]
sum(tabla_prob[1:5])
pnorm(0,0,3)
qnorm(0,0,3)
pnorm(-2, mean = 0, sd=2)
$ brew install git
load("~/Library/CloudStorage/OneDrive-Personal/Documentos/UNIVERSIDAD/NOVENO SEMESTRE/Seminario de Investigación Política - D/WE.2.RData")
library(e1071)
# Eliminar GEO_ID de los conjuntos de datos si no es relevante
dataTrain <- dataTrain[, -which(names(dataTrain) == "GEO_ID")]
dataTest <- dataTest[, -which(names(dataTest) == "GEO_ID")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "NAME")]
dataTest <- dataTest[, -which(names(dataTest) == "NAME")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "STATE")]
dataTest <- dataTest[, -which(names(dataTest) == "STATE")]
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(x_train))
library(caret)
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(x_train))
# Dividir los datos en conjunto de entrenamiento y prueba
set.seed(42)  # Para reproducibilidad
trainIndex1 <- createDataPartition(dt$repsh20, p = .7,
list = FALSE,
times = 1)
dataTrain <- dt[ trainIndex,]
dataTest  <- dt[-trainIndex,]
# Convertir los datos a matrices
x_train <- as.matrix(dataTrain[, -which(names(dataTrain) == "repsh20")])
y_train <- dataTrain$repsh20
x_test <- as.matrix(dataTest[, -which(names(dataTest) == "repsh20")])
y_test <- dataTest$repsh20
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(x_train))
# Imprimir el modelo
print(svm_model)
# Hacer predicciones en el conjunto de prueba
predictions <- predict(svm_model, newdata = dataTest)
dataTrain <- dataTrain[, -which(names(dataTrain) == "GEO_ID")]
dataTest <- dataTest[, -which(names(dataTest) == "GEO_ID")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "NAME")]
dataTest <- dataTest[, -which(names(dataTest) == "NAME")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "STATE")]
dataTest <- dataTest[, -which(names(dataTest) == "STATE")]
# Imprimir el modelo
print(svm_model)
# Hacer predicciones en el conjunto de prueba
predictions <- predict(svm_model, newdata = dataTest)
# Hacer predicciones en el conjunto de prueba
predictions <- predict(svm_model, newdata = dataTest)
View(dataTrain)
dataTest <- dataTest[, -which(names(dataTest) == "GEO_ID")]
# Hacer predicciones en el conjunto de prueba
predictions <- predict(svm_model, newdata = dataTest)
# Eliminar columnas irrelevantes
data2 <- dt[, !names(dt) %in% c("GEO_ID", "NAME", "STATE")]
# Dividir los datos en conjunto de entrenamiento y prueba
set.seed(42)  # Para reproducibilidad
trainIndex <- createDataPartition(data2$repsh20, p = .7,
list = FALSE,
times = 1)
dataTrain <- data2[trainIndex,]
dataTest <- data2[-trainIndex,]
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(x_train))
# Imprimir el modelo
print(svm_model)
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(datatrain))
# Ajustar el modelo SVM con kernel radial
svm_model <- svm(repsh20 ~ ., data = dataTrain,
kernel = "radial",
cost = 1,
gamma = 1/ncol(dataTrain))
# Imprimir el modelo
print(svm_model)
# Hacer predicciones en el conjunto de prueba
predictions <- predict(svm_model, newdata = dataTest)
# Calcular el error cuadrático medio (MSE)
mse_test <- mean((y_test - predictions)^2)
print(paste("Mean Squared Error (Test):", mse_test))
# Calcular el R²
r_squared_test <- 1 - sum((y_test - predictions)^2) / sum((y_test - mean(y_test))^2)
print(paste("R² (Test):", r_squared_test))
# Búsqueda de hiperparámetros óptimos usando validación cruzada
tune_result <- tune(svm, repsh20 ~ ., data = dataTrain,
ranges = list(cost = 10^(-1:2), gamma = 10^(-2:1)))
tune_result <- tune(svm, repsh20 ~ ., data = dataTrain,
ranges = list(cost = c(0.1, 1, 10), gamma = c(0.01, 0.1, 1)))
train_control <- trainControl(method = "cv", number = 3)  # Usa 3 particiones en lugar de 10
tune_result <- tune(svm, repsh20 ~ ., data = dataTrain,
ranges = list(cost = c(0.1, 1, 10), gamma = c(0.01, 0.1, 1)),
tunecontrol = train_control)
# Configuración de la validación cruzada con menos particiones
train_control <- trainControl(method = "cv", number = 3)  # Usa 3 particiones en lugar de 10
# Búsqueda de hiperparámetros óptimos
tune_result <- tune(svm, repsh20 ~ ., data = dataTrain,
ranges = list(cost = c(0.1, 1, 10), gamma = c(0.01, 0.1, 1)),
tunecontrol = train_control)
library(neuralnet)
# ==== Redes Neuronales====
install.packages("neuralnet")
library(neuralnet)
library(caret)
# Dividir los datos en conjunto de entrenamiento y prueba
set.seed(42)  # Para reproducibilidad
trainIndex <- createDataPartition(dt$repsh20, p = .7,
list = FALSE,
times = 1)
dataTrain <- data[ trainIndex,]
dataTest  <- data[-trainIndex,]
# Normalizar los datos
maxs <- apply(dataTrain, 2, max)
mins <- apply(dataTrain, 2, min)
scaled_dataTrain <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
str(dataTrain)  # Revisa la estructura de dataTrain
scaled_dataTrain <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
dataTrain <- dataTrain[, -which(names(dataTrain) == "GEO_ID")]
dataTest <- dataTest[, -which(names(dataTest) == "GEO_ID")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "NAME")]
dataTest <- dataTest[, -which(names(dataTest) == "NAME")]
dataTrain <- dataTrain[, -which(names(dataTrain) == "STATE")]
dataTest <- dataTest[, -which(names(dataTest) == "STATE")]
maxs <- apply(dataTrain, 2, max)
mins <- apply(dataTrain, 2, min)
scaled_dataTrain <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
scaled_dataTest <- as.data.frame(scale(dataTest, center = mins, scale = maxs - mins))
# Ajustar el modelo de redes neuronales
nn <- neuralnet(repsh20 ~ edu + emp + mex + mex2 + pov + gin + asis + rur + rel + ppi + repsh80,
data = scaled_dataTrain,
hidden = c(5, 3),  # Estructura de la red (2 capas ocultas con 5 y 3 neuronas)
linear.output = TRUE)
View(scaled_dataTrain)
View(dataTrain)
colnames(scaled_dataTrain)
numeric_cols <- sapply(dataTrain, is.numeric)
dataTrain_numeric <- dataTrain[, numeric_cols]
maxs <- apply(dataTrain_numeric, 2, max, na.rm = TRUE)
mins <- apply(dataTrain_numeric, 2, min, na.rm = TRUE)
scaled_dataTrain <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
scaled_dataTest <- as.data.frame(scale(dataTest, center = mins, scale = maxs - mins))
colnames(scaled_dataTrain)
nn <- neuralnet(repsh20 ~ edu + emp + mex + mex2 + pov + gin + asis + rur + rel + ppi + repsh80,
data = scaled_dataTrain,
hidden = c(5, 3),  # Estructura de la red (2 capas ocultas con 5 y 3 neuronas)
linear.output = TRUE)
View(scaled_dataTrain)
dataTrain <- dt[ trainIndex,]
dataTest  <- dt[-trainIndex,]
numeric_cols <- sapply(dataTrain, is.numeric)
dataTrain_numeric <- dataTrain[, numeric_cols]
maxs <- apply(dataTrain_numeric, 2, max, na.rm = TRUE)
mins <- apply(dataTrain_numeric, 2, min, na.rm = TRUE)
scaled_dataTrain <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
colnames(scaled_dataTrain)
scaled_dataTest <- as.data.frame(scale(dataTest, center = mins, scale = maxs - mins))
numeric_cols <- sapply(dataTrain, is.numeric)
dataTrain_numeric <- dataTrain[, numeric_cols]
maxs <- apply(dataTrain_numeric, 2, max, na.rm = TRUE)
mins <- apply(dataTrain_numeric, 2, min, na.rm = TRUE)
scaled_dataTrain <- as.data.frame(scale(dataTrain_numeric, center = mins, scale = maxs - mins))
scaled_dataTest <- as.data.frame(scale(dataTest, center = mins, scale = maxs - mins))
numeric_cols1 <- sapply(dataTest, is.numeric)
dataTest_numeric <- dataTest[, numeric_cols1]
scaled_dataTest <- as.data.frame(scale(dataTest_numeric, center = mins, scale = maxs - mins))
nn <- neuralnet(repsh20 ~ edu + emp + mex + mex2 + pov + gin + asis + rur + rel + ppi + repsh80,
data = scaled_dataTrain,
hidden = c(5, 3),  # Estructura de la red (2 capas ocultas con 5 y 3 neuronas)
linear.output = TRUE)
any(is.na(scaled_dataTrain))
scaled_dataTrain <- na.omit(scaled_dataTrain)
any(is.infinite(scaled_dataTrain))
# Ajustar el modelo de redes neuronales
nn <- neuralnet(repsh20 ~ edu + emp + mex + mex2 + pov + gin + asis + rur + rel + ppi + repsh80,
data = scaled_dataTrain,
hidden = c(5, 3),  # Estructura de la red (2 capas ocultas con 5 y 3 neuronas)
linear.output = TRUE)
stargazer(RLM1, RLM2, RLM3, type = "latex",
title = "Modelos de Regresión",
dep.var.labels = c("M1", "M2", "M3"),
covariate.labels = c("Determinantes del voto por Trump"),
out = "resultados_regresiones.tex")
library(stargazer)
stargazer(RLM1, RLM2, RLM3, type = "latex",
title = "Modelos de Regresión",
dep.var.labels = c("M1", "M2", "M3"),
covariate.labels = c("Determinantes del voto por Trump"),
out = "resultados_regresiones.tex")
plot(RLM1)
View(Vardep_merged)
View(Var_data_v2)
View(Var_merged)
View(Merged18)
View(data)
View(data1)
View(dt)
View(Merged18)
View(Vardep_merged)
data <- data.frame(
a[1:20]
b[c(0,1)]
data <- data.frame(
a[1:20],
b[c(0,1)],
c[100:110]
)
data <- data.frame(
a=1:20,
b=c(0,1),
c=100:110
)
#create a dataframe
df <- data.frame(
RacePosition = 1:5,
WayTheySayHi = as.factor(c('Hi','Hello','Hey','Yo','Hi')),
NumberofKids = c(3,5,1,0,2))
#create a dataframe
df <- data.frame(
RacePosition = 1:5,
WayTheySayHi = as.factor(c('Hi','Hello','Hey','Yo','Hi')),
NumberofKids = c(3,5,1,0,2))
View(df)
#view the data frame
df
#view the data like a spreadhseet using the Rstudio viewer
View(df)
#check out the structure of the data
str(df)
#You can call a column with "$" and you get a vector, which you can treat like any vector
df$NumberofKids
df$NumberofKids[2]
df$NumberofKids >= 3
#Many ways to call a column
df$NumberofKids >= 3
df[,3] >= 3
df[,'NumberofKids']>=3
#Run some calculations on different vectors
mean(df$RacePosition)
df$WayTheySayHi[4]
sum(df$NumberofKids <= 1)
df2 <- data.frame(a = 1:20,
b = 0:19*2,
c = sample(101:200,20,replace=TRUE))
View(df)
df2
View(df2)
mean(df2$c)
sum(df2$a*df2$b)
sum(df2$c <= 103) > 0
df2$b[8]
sum(df2$b > 10 & df2$c < 150)
#Creating a new variable
df$State <- c('Alaska','California',
'California','Maine',
'Florida')
df
#Install tidiverse
#install.packages('tidyverse')
library(tidyverse)
df1 <- df %>% mutate(State = c('Alaska','California',
'California','Maine','Florida'))
df2 <- mutate(df,State = c('Alaska','California',
'California','Maine','Florida'))
#check they are indeed the same
identical(df1,df2)
#keep one of them, doesnt matter which one
df <- df1
#keep one of them, doesnt matter which one
df <- df1
df <- df %>% mutate(
MoreThanTwoKids = NumberofKids > 2,
One = 1,
KidsPlusPosition = NumberofKids + RacePosition)
df
#manipulating some variables
df <- df %>%
select(-KidsPlusPosition,-WayTheySayHi,-One) %>%
mutate(State = as.factor(State),
RacePosition = RacePosition - 1)
df$State[3] <- 'Alaska'
str(df)
#Renaming variaables
names(df)
df <- df %>% rename(Pos = RacePosition, Num.Kids=NumberofKids,
mt2Kids = MoreThanTwoKids)
names(df)
data <- data.frame(a = 1:10*2,
b = c(0,1),
c = sample(1:100,10,replace=FALSE)) %>%
rename(EvenNumbers = a, Treatment = b, Outcome = c)
data <- data %>%
mutate(Big = EvenNumbers > 15,
Outcome = Outcome + Treatment,
AboveMean = Outcome > mean(Outcome))
View(data)
setwd:("/Users/alvaroperezlopez/Desktop/Curso R/DATA")
setwd("/Users/alvaroperezlopez/Desktop/Curso R/DATA")
pib <- read_csv("\Users\alvaroperezlopez\Desktop\Crecimiento del PIB per cápita (% anual).csv")
pib <- read_csv("Crecimiento del PIB per cápita (%anual).csv")
library(tidyverse)
pib <- read_csv("Crecimiento del PIB per cápita (%anual).csv")
pib <- read_csv("Crecimiento del PIB per cápita (% anual).csv")
View(pib)
str(pib)
head(pib)
pib <- read_csv("Crecimiento del PIB per cápita (% anual).xlsx")
View(pib)
library(readxl)
Crecimiento_del_PIB_per_ca_pita_anual_ <- read_excel("Crecimiento del PIB per cápita (% anual).xlsx")
View(Crecimiento_del_PIB_per_ca_pita_anual_)
pib <- Crecimiento_del_PIB_per_ca_pita_anual_
str(pib)
head(pib)
pib <- (Crecimiento_del_PIB_per_ca_pita_anual_, skip=3,stringsAsFactors = F)
pib <- read_xlsx(Crecimiento_del_PIB_per_ca_pita_anual_, skip=3,stringsAsFactors = F)
View(Crecimiento_del_PIB_per_ca_pita_anual_)
library(readxl)
Crecimiento_del_PIB_per_ca_pita_anual_ <- read_excel("Crecimiento del PIB per cápita (% anual).xlsx")
View(Crecimiento_del_PIB_per_ca_pita_anual_)
View(Crecimiento_del_PIB_per_ca_pita_anual_)
pib <- Crecimiento_del_PIB_per_ca_pita_anual_
str(pib)
head(pib)
